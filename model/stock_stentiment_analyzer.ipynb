{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFLTFbGYQH3O"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "# Positives and Negatives from this dataset\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"yash612/stockmarket-sentiment-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9GapQB4QIGj"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "# Neutrals from this dataset\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sbhatti/financial-sentiment-analysis\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdR7iY7N7f3l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import os\n",
        "\n",
        "path1 = \"/root/.cache/kagglehub/datasets/yash612/stockmarket-sentiment-dataset/versions/1\"\n",
        "df1 = pd.read_csv(os.path.join(path1, \"stock_data.csv\"))\n",
        "\n",
        "# 1 = positive, -1 = negative\n",
        "df1['sentiment'] = df1['Sentiment'].map({1: 'positive', -1: 'negative'})\n",
        "df1['text'] = df1['Text']\n",
        "df1 = df1[['text', 'sentiment']]\n",
        "\n",
        "# Extract the neutral data from the second dataset\n",
        "\n",
        "path2 = \"/root/.cache/kagglehub/datasets/sbhatti/financial-sentiment-analysis/versions/4\"\n",
        "df2 = pd.read_csv(os.path.join(path2, \"data.csv\"))\n",
        "\n",
        "df2['text'] = df2['Sentence']\n",
        "df2['sentiment'] = df2['Sentiment'].str.lower()\n",
        "df2_neutral = df2[df2['sentiment'] == 'neutral'].copy()\n",
        "df2_neutral = df2_neutral[['text', 'sentiment']]\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "    # Remove urls, user tags, and whitespace\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "df1['text'] = df1['text'].apply(clean_text)\n",
        "df2_neutral['text'] = df2_neutral['text'].apply(clean_text)\n",
        "\n",
        "# Remove sentences shorter than 10 characters\n",
        "df1 = df1[df1['text'].str.len() >= 10]\n",
        "df2_neutral = df2_neutral[df2_neutral['text'].str.len() >= 10]\n",
        "\n",
        "# Combine the datasets\n",
        "combined_df = pd.concat([df1, df2_neutral], ignore_index=True)\n",
        "\n",
        "# Remove duplicates\n",
        "before_dedup = len(combined_df)\n",
        "combined_df = combined_df.drop_duplicates(subset=['text'], keep='first')\n",
        "after_dedup = len(combined_df)\n",
        "\n",
        "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "combined_df['label'] = combined_df['sentiment'].map(label_map)\n",
        "\n",
        "# Balance the classes\n",
        "\n",
        "# Check current distribution\n",
        "class_counts = combined_df['label'].value_counts()\n",
        "# print(f\"Current distribution:\")\n",
        "for label, count in class_counts.items():\n",
        "    sentiment = list(label_map.keys())[list(label_map.values()).index(label)]\n",
        "    print(f\"  {sentiment}: {count}\")\n",
        "\n",
        "# Undersample to match the number samples in neutral\n",
        "min_count = class_counts.min()\n",
        "\n",
        "balanced_dfs = []\n",
        "for label in combined_df['label'].unique():\n",
        "    label_df = combined_df[combined_df['label'] == label]\n",
        "    sampled = label_df.sample(n=min(len(label_df), min_count), random_state=42)\n",
        "    balanced_dfs.append(sampled)\n",
        "\n",
        "balanced_df = pd.concat(balanced_dfs, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split into validation, test, and train\n",
        "\n",
        "# Test set (15%)\n",
        "train_val, test = train_test_split(\n",
        "    balanced_df,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=balanced_df['label']\n",
        ")\n",
        "\n",
        "# Validation (15%)\n",
        "val_size_adjusted = 0.15 / 0.85\n",
        "train, val = train_test_split(\n",
        "    train_val,\n",
        "    test_size=val_size_adjusted,\n",
        "    random_state=42,\n",
        "    stratify=train_val['label']\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(train)} samples\")\n",
        "print(f\"Validation set: {len(val)} samples\")\n",
        "print(f\"Test set: {len(test)} samples\")\n",
        "\n",
        "# save the data\n",
        "output_dir = 'data/processed'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "train.to_csv(f'{output_dir}/train.csv', index=False)\n",
        "val.to_csv(f'{output_dir}/val.csv', index=False)\n",
        "test.to_csv(f'{output_dir}/test.csv', index=False)\n",
        "\n",
        "balanced_df.to_csv(f'{output_dir}/balanced_full.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbGClSgbA9Fk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# load the data\n",
        "train_df = pd.read_csv('data/processed/train.csv')\n",
        "val_df = pd.read_csv('data/processed/val.csv')\n",
        "test_df = pd.read_csv('data/processed/test.csv')\n",
        "\n",
        "# Model initialization\n",
        "model_name = 'ProsusAI/finbert'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = SentimentDataset(train_df['text'].values, train_df['label'].values, tokenizer)\n",
        "val_dataset = SentimentDataset(val_df['text'].values, val_df['label'].values, tokenizer)\n",
        "test_dataset = SentimentDataset(test_df['text'].values, test_df['label'].values, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# training\n",
        "epochs = 3\n",
        "lr = 2e-5\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    progress_bar = tqdm(loader, desc='Training')\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions, true_labels, confidences = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc='Evaluating'):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            conf = torch.max(probs, dim=1)[0]\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            confidences.extend(conf.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return avg_loss, accuracy, predictions, true_labels, confidences\n",
        "\n",
        "print(\"Training starting\")\n",
        "\n",
        "best_val_acc = 0\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc, _, _, _ = evaluate(model, val_loader, device)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'finbert_best_model.pt')\n",
        "        print(f\"accuracy: {val_acc:.4f}\")\n",
        "\n",
        "print(\"TRAINING COMPLETE\")\n",
        "\n",
        "# Test evaluation\n",
        "\n",
        "model.load_state_dict(torch.load('finbert_best_model.pt'))\n",
        "test_loss, test_acc, preds, labels, confs = evaluate(model, test_loader, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Average Confidence: {np.mean(confs):.4f}\")\n",
        "\n",
        "# Classification report\n",
        "label_names = ['negative', 'neutral', 'positive']\n",
        "print(f\"\\n{classification_report(labels, preds, target_names=label_names)}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=label_names, yticklabels=label_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Training history plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "ax1.plot(history['val_loss'], label='Val Loss', marker='o')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
        "ax2.plot(history['val_acc'], label='Val Accuracy', marker='o')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "model.save_pretrained('finbert_sentiment_model')\n",
        "tokenizer.save_pretrained('finbert_sentiment_model')\n",
        "\n",
        "# test predictions\n",
        "\n",
        "def predict_sentiment(text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = torch.softmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
        "        pred = np.argmax(probs)\n",
        "        confidence = probs[pred]\n",
        "\n",
        "    sentiment = label_names[pred]\n",
        "    return sentiment, confidence, probs\n",
        "\n",
        "examples = [\n",
        "    \"Apple stock soars after beating earnings expectations!\",\n",
        "    \"The company reported disappointing quarterly results today.\",\n",
        "    \"Stock prices remained steady with no major changes.\",\n",
        "]\n",
        "\n",
        "for text in examples:\n",
        "    sentiment, confidence, probs = predict_sentiment(text, model, tokenizer, device)\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Prediction: {sentiment.upper()} ({confidence:.2%} confidence)\")\n",
        "    print(f\"Probabilities: Neg={probs[0]:.2%}, Neu={probs[1]:.2%}, Pos={probs[2]:.2%}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}